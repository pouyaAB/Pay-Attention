import math
import numpy as np
import chainer
import chainer.functions as F
import chainer.links as L
from chainer import cuda, Variable
from chainer.initializers import Normal


class Discriminator_cond(chainer.Chain):
    """
    A discriminator for classifying the both the regular and masked images to real or fake categories.
    The discrimiantor can receive both regular and masked images as input and it will try to 
    classify them based on the object of interest in the image. The discriminator will either 
    mark the image as fake or real.

    The regular and masked images have will go through different convolution and FF layers at
    the end. The first 7 convoltion layers are shared and there is two additional convolution 
    layers for each regular and masked images.

    The discriminator will also receive the task encodings as input. Like the model described in
    'CVAE-GAN: Fine-Grained Image Generation through Asymmetric Training' by Jianmin Bao et.al.

    Intent: Providing the task_encodings as input to the discriminator can push the system to find
    the relevant object better since it will classify the masked images with incorrect task encodings
    as fake.

    The discriminator will be used in an adversarial setup with the encoder and the generator.
    The discriminator tries to mark the images generated by the generator as fake and classify 
    real images to their correct class.

    Validation: One can check the classification error for real and fake images
    """
    def __init__(self, density=1, size=64, channel=3, num_objects=10, num_describtions=10):
        assert (size % 16 == 0)
        second_size = size / 4
        initial_size = size / 16
        super(Discriminator_cond, self).__init__(
            # shared layers between regualr and masked images
            dc1=L.Convolution2D(channel, int(16 * density), 3, stride=2, pad=1,
                                initialW=Normal(0.02)),
            dc2=L.Convolution2D(int(16 * density), int(32 * density), 3, stride=2, pad=1,
                                initialW=Normal(0.02)),
            dc1_=L.Convolution2D(int(16 * density), int(16 * density), 3, stride=1, pad=1,
                                initialW=Normal(0.02)),
            # extra layers added to make it deeper with stride = 1
            dc2_=L.Convolution2D(int(32 * density), int(32 * density), 3, stride=1, pad=1,
                                initialW=Normal(0.02)),
            norm2=L.BatchNormalization(int(32 * density)),
            norm2_=L.BatchNormalization(int(32 * density)),
            # "plus layer" another extra layer added to make it deeper with stride = 1 but this one has 
            # a skip connection between input and output
            dc2_p=L.Convolution2D(int(32 * density), int(32 * density), 3, stride=1, pad=1,
                                initialW=Normal(0.02)),
            norm2_p=L.BatchNormalization(int(32 * density)),
            dc3=L.Convolution2D(int(32 * density + 7), int(64 * density), 3, stride=2, pad=1,
                                initialW=Normal(0.02)),
            norm3=L.BatchNormalization(int(64 * density)),
            dc3_=L.Convolution2D(int(64 * density), int(64 * density), 3, stride=1, pad=1,
                                initialW=Normal(0.02)),
            norm3_=L.BatchNormalization(int(64 * density)),

            #seperated layers for the regular images
            dc3_p=L.Convolution2D(int(64 * density), int(64 * density), 3, stride=1, pad=1,
                                initialW=Normal(0.02)),
            norm3_p=L.BatchNormalization(int(64 * density)),
            dc4=L.Convolution2D(int(64 * density), int(128 * density), 3, stride=2, pad=1,
                                initialW=Normal(0.02)),
            norm4=L.BatchNormalization(int(128 * density)),

            # seperated layers for the masked images
            dc3_p_att=L.Convolution2D(int(64 * density), int(64 * density), 3, stride=1, pad=1,
                                initialW=Normal(0.02)),
            norm3_p_att=L.BatchNormalization(int(64 * density)),
            dc4_att=L.Convolution2D(int(64 * density), int(128 * density), 3, stride=2, pad=1,
                                initialW=Normal(0.02)),
            norm4_att=L.BatchNormalization(int(128 * density)),

            toConv=L.Linear(num_objects + num_describtions, second_size * second_size * 7, initialW=Normal(0.02)),
            FC1=L.Linear(initial_size * initial_size * int(128 * density), 2, initialW=Normal(0.02)),
            FC2=L.Linear(initial_size * initial_size * int(128 * density), 2, initialW=Normal(0.02))
        )

    def __call__(self, x, objects_one_hot, descs_one_hot, att=True, train=True):
        with chainer.using_config('train', train), chainer.using_config('enable_backprop', train):
            xp = cuda.get_array_module(x.data)
            # h0 = F.concat((x, objects, descs), axis=1)
            h0 = self.toConv(F.concat((objects_one_hot, descs_one_hot), axis=-1))
            h0 = F.reshape(h0, (h0.shape[0], 7, 32, 32))
            h1 = F.leaky_relu(self.dc1(x))
            h1_ = F.leaky_relu(self.dc1_(h1))
            h2 = F.leaky_relu(self.norm2(self.dc2(h1_)))
            h2_ = F.leaky_relu(self.norm2_(self.dc2_(h2)))
            h2_p = F.leaky_relu(self.norm2_p(self.dc2_p(h2_)))
            h2_ = h2_ + h2_p
            h2_ = F.concat((h2_, h0), axis=1)
            h3 = F.leaky_relu(self.norm3(self.dc3(h2_)))
            h3_ = F.leaky_relu(self.norm3_(self.dc3_(h3)))
            if att:
                h3_p = F.leaky_relu(self.norm3_p_att(self.dc3_p_att(h3)))
                h3_ = h3_ + h3_p
                h4 = F.leaky_relu(self.norm4_att(self.dc4_att(h3_)))
                return self.FC1(h4), h3_p
            else:
                h3_p = F.leaky_relu(self.norm3_p(self.dc3_p(h3)))
                h3_ = h3_ + h3_p
                h4 = F.leaky_relu(self.norm4(self.dc4(h3_)))
                return self.FC2(h4), h3_p